{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "949c6c33-8b89-4ed8-afb8-64452d81200e",
   "metadata": {},
   "source": [
    "# Tarefa 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87643098-8511-4777-b117-c3f164dcdf91",
   "metadata": {},
   "source": [
    "### 1. Cite 5 diferenças entre o Adaboost e o GBM.\n",
    "\n",
    "#### 1. Peso nas Instâncias:\n",
    "- ##### **AdaBoost** :\n",
    "  Atribui pesos às instâncias de treinamento, dando mais peso às instâncias mal classificadas em iterações anteriores.\n",
    "O foco é corrigir os erros cometidos pelos modelos anteriores, aumentando a importância das instâncias difíceis de classificar.\n",
    "- ##### **GBM** :\n",
    "  Atualiza os pesos das instâncias de treinamento, mas o foco é minimizar o gradiente da função de perda em relação às previsões do modelo atual.\n",
    "Ajusta os resíduos do modelo anterior, em vez de apenas aumentar o peso das instâncias mal classificadas.\n",
    "\n",
    "#### 2. Flexibilidade:\n",
    "- ##### **AdaBoost**:\n",
    "   É mais restrito a problemas de classificação binária, embora existam extensões para classificação multiclasse.\n",
    "- ##### **GBM** :\n",
    "   É mais flexível, podendo ser aplicado a problemas de regressão, classificação binária e multiclasse, dependendo da função de perda escolhida.\n",
    "\n",
    "#### 3.Modelos Base:\n",
    "- ##### **AdaBoost** :\n",
    "  Geralmente utiliza modelos mais simples, como árvores de decisão rasas.\n",
    "- ##### **GBM** :\n",
    "  Pode usar uma variedade de modelos base, mas geralmente utiliza árvores de decisão mais profundas.\n",
    "\n",
    "#### 4.Sensibilidade a Outliers:\n",
    "- ##### **AdaBoost** :\n",
    "  Pode ser sensível a outliers, pois os modelos subsequentes focam em corrigir erros, incluindo outliers.\n",
    "Outliers podem receber pesos excessivos, prejudicando o desempenho do modelo.\n",
    "- ##### **GBM** :\n",
    "  Menos sensível a outliers devido à abordagem de minimização de gradientes, mas ainda pode ser influenciado por eles, especialmente em estágios iniciais.\n",
    "\n",
    "#### 5.Treinamento Sequencial vs. Paralelo:\n",
    "- ##### **AdaBoost** :\n",
    "  Treina modelos sequencialmente, ajustando os pesos das instâncias em cada iteração.\n",
    "- ##### **GBM** :\n",
    "  Também treina modelos sequencialmente, mas cada novo modelo é treinado para corrigir os resíduos do modelo anterior, resultando em um treinamento mais iterativo e sequencial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10366be6-e900-4a38-a867-8899fc57a625",
   "metadata": {},
   "source": [
    "### 2. Acesse o link Scikit-learn–adaboost, leia a explicação (traduza se for preciso) e crie um jupyternotebook contendo o exemplo do AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6d769aa-0fd2-4e07-9016-433b34682bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics  import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3e9670c-fe4a-438d-9ff9-6403e91b3c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100,\n",
    "                                 learning_rate=1.0,\n",
    "                                 max_depth=1,\n",
    "                                 random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f61807bd-774d-466b-b967-6a8f2cc4bf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960318"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "\n",
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state= 42, loss='squared_error').fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f7ed02-6731-4b58-ab17-dbc2b6a193c0",
   "metadata": {},
   "source": [
    "### 3. Cite 5 Hyperparametros importantes no GBM.\n",
    "\n",
    "#### 1.Número de Estimadores (`n_estimators`) :\n",
    "- Define quantas árvores (ou modelos base) serão treinados.\n",
    "Aumentar o número de estimadores pode aumentar o desempenho do modelo, mas também pode levar a um aumento no tempo de treinamento.\n",
    "\n",
    "#### 2.Número Mínimo de Amostras em Folhas (`min_samples_leaf`) :\n",
    "- Define o número mínimo de amostras necessárias para criar uma folha. Valores maiores ajudam a evitar folhas com poucas amostras, o que pode reduzir o overfitting e melhorar a generalização.\n",
    "\n",
    "#### 3.Subamostragem de Características (`subsample`) :\n",
    "- controla a fração dos dados de treinamento usada para treinar cada árvore do modelo.\n",
    "\n",
    "#### 4.Profundidade Máxima das Árvores (`max_depth`) :\n",
    "- Controla a profundidade máxima de cada árvore. Árvores mais profundas capturam padrões mais complexos, mas aumentam o risco de overfitting.\n",
    "\n",
    "#### 5.Taxa de Aprendizado (`learning_rate`) :\n",
    "- Define o peso de cada árvore no modelo. Valores menores exigem mais árvores para convergir, mas podem melhorar a generalização. Valores maiores aceleram o treinamento, mas podem levar a overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34cb83-c494-4d8e-85a2-2c2bc8d7aba3",
   "metadata": {},
   "source": [
    "### 4. (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53d0bd09-370b-4eb0-8eda-6a96db207685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89a9e820-1f1a-4f23-a0d3-25104e33e217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.2 s, sys: 110 ms, total: 37.4 s\n",
      "Wall time: 37.4 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1</td>\n",
       "      <td>3.625209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1</td>\n",
       "      <td>3.667692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>3.754018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>3.754086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>3.794056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1</td>\n",
       "      <td>21.909960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9</td>\n",
       "      <td>22.398583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6</td>\n",
       "      <td>22.471035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>22.902975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>24.252178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  learning_rate  max_depth  mean_squared_error\n",
       "0           1000           0.03          1            3.625209\n",
       "1           1000           0.06          1            3.667692\n",
       "2          10000           0.10          3            3.754018\n",
       "3           1000           0.10          3            3.754086\n",
       "4            100           0.10          3            3.794056\n",
       "..           ...            ...        ...                 ...\n",
       "59            10           0.03          1           21.909960\n",
       "60            10           0.01          9           22.398583\n",
       "61            10           0.01          6           22.471035\n",
       "62            10           0.01          3           22.902975\n",
       "63            10           0.01          1           24.252178\n",
       "\n",
       "[64 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimators     = [10, 100, 1000, 10000]\n",
    "learning_rates = [0.01, 0.03, 0.06, 0.1]\n",
    "max_depths     = [1, 3, 6, 9]\n",
    "\n",
    "grid_search = []\n",
    "\n",
    "for n in estimators:\n",
    "    for rate in learning_rates:\n",
    "        for depth in max_depths:\n",
    "            est = GradientBoostingRegressor(n_estimators=n, learning_rate=rate, max_depth=depth, random_state=0, loss='squared_error').fit(X_train, y_train)\n",
    "            grid_search.append([n, rate, depth, mean_squared_error(y_test, est.predict(X_test))])\n",
    "\n",
    "(pd.DataFrame(data=grid_search, columns=['n_estimators', 'learning_rate', 'max_depth', 'mean_squared_error'])\n",
    "   .sort_values(by='mean_squared_error', ascending=True, ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb64015-0742-4037-9482-316a6cca140c",
   "metadata": {},
   "source": [
    "### 5. Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?\n",
    "\n",
    "A maior diferença é que o Stochastic GBM adiciona aleatoriedade ao treinamento, usando subamostragem de amostras ou características, enquanto o GBM tradicional usa todos os dados disponíveis. Essa aleatoriedade melhora a generalização e reduz o overfitting. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
