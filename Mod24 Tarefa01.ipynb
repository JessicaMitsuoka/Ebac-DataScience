{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e26edb-5caf-4f4d-9edd-c2461457f7fb",
   "metadata": {},
   "source": [
    "# Tarefa3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77529af-37c6-4d71-bf81-150059eaf45c",
   "metadata": {},
   "source": [
    "## 1. Cite 5 diferenças entre o Random Forest e o Adaboost\n",
    "\n",
    "### 1. Tamanho das Árvores:\n",
    "- ####  Random Forest:\n",
    "  As árvores são construídas de forma independente e geralmente têm tamanho completo (sem poda ou limitação de profundidade máxima).\n",
    "O tamanho das árvores pode variar, dependendo dos dados e das divisões feitas durante o treinamento.\n",
    "\n",
    "- #### AdaBoost:\n",
    "  As árvores são geralmente compostas apenas por um nó e duas folhas (conhecidas como \"stumps\"). Portanto, um conjunto de árvores AdaBoost é, na verdade, um conjunto de \"stumps\".\n",
    "\n",
    "### 2. Precisão na Classificação:\n",
    "- #### Random Forest:\n",
    "  Utiliza todas as variáveis disponíveis para tomar decisões em cada divisão da árvore.\n",
    "Geralmente é mais preciso devido à combinação de múltiplas árvores complexas.\n",
    "\n",
    "- #### AdaBoost:\n",
    "  Cada \"stump\" usa apenas uma variável para tomar decisões, o que o torna um \"aprendiz fraco\".\n",
    "\n",
    "### 3. Distribuição de Votos:\n",
    "- #### Random Forest:\n",
    "  Cada árvore tem peso igual na classificação final.\n",
    "\n",
    "- #### AdaBoost:\n",
    "  Cada \"stump\" tem um peso diferente na decisão final, dependendo de seu desempenho durante o treinamento.\n",
    "\n",
    "### 4. Ordem de Construção das Árvores:\n",
    "- #### Random Forest:\n",
    "  As árvores são construídas de forma independente e paralela\n",
    "\n",
    "- #### AdaBoost:\n",
    "  A ordem de construção é importante: cada \"stump\" é treinado para corrigir os erros do modelo anterior.\n",
    "Os erros cometidos pelas primeiras \"stumps\" influenciam o treinamento das subsequentes.\n",
    "\n",
    "\n",
    "### 5. Tipo de Algoritmo:\n",
    "- #### Random Forest:\n",
    "  É um algoritmo de bagging (Bootstrap Aggregating), que combina múltiplas árvores de decisão treinadas em subconjuntos aleatórios dos dados.\n",
    "\n",
    "- #### AdaBoost:\n",
    "  É um algoritmo de boosting, que combina muitos \"aprendizes fracos\" de forma sequencial, ajustando-se aos erros anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e111202-603f-4e52-a64f-f3b49183d13d",
   "metadata": {},
   "source": [
    "## 2. Acesse o link Scikit-learn– adaboost , leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo do AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2034cd70-a439-4fca-9b08-40cade0de6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533333333333334"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\",)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b2270f-d0a8-4dba-8145-fdd81336ccbb",
   "metadata": {},
   "source": [
    "## 3. Cite 5 Hyperparametros importantes no AdaBoost.\n",
    "\n",
    "#### 1. `n_estimators`:\n",
    "- Controla o número de \"stumps\" (árvores de decisão de profundidade 1) ou outros modelos fracos que serão combinados no ensemble.\n",
    "- Quanto maior o número de estimadores, mais complexo o modelo se torna, mas também pode levar ao overfitting.\n",
    "\n",
    "#### 2. `learning_rate`:\n",
    "- Define o peso atribuído a cada classificador em cada iteração do boosting.\n",
    "- Uma taxa de aprendizado alta faz com que cada classificador tenha mais influência, o que pode acelerar o aprendizado, mas também aumentar o risco de overfitting.\n",
    "\n",
    "#### 3. `algorithm`:\n",
    "-  Define a variante do AdaBoost a ser usada.\n",
    "-  O 'SAMME.R' usa probabilidades de classe e geralmente converge mais rapidamente, enquanto o 'SAMME' é discreto e é mais geral, funcionando com qualquer tipo de classificador fraco.\n",
    "\n",
    "#### 4. `random_state`:\n",
    "- Controla a aleatoriedade na inicialização dos estimadores, garantindo que os resultados sejam reproduzíveis.\n",
    "- Útil para reproduzir resultados consistentes.\n",
    "\n",
    "#### 5. `base_estimator`:\n",
    "- Define o tipo de estimador fraco que será usado no AdaBoost. O padrão é um `DecisionTreeClassifier` com `max_depth=1`.\n",
    "- Pode ser personalizado para outros estimadores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0aecc5-a30d-4cbc-9166-dc82f682dec9",
   "metadata": {},
   "source": [
    "## 4. (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo (load_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32ca9156-5b16-4971-818a-3f9a2f769448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a716113-71a0-42fe-b3ac-e91d9ffa9fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>mean_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>301</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>401</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>501</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>601</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>701</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>801</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1001</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  mean_score\n",
       "0              1    0.666667\n",
       "1            101    0.960000\n",
       "2            201    0.953333\n",
       "3            301    0.953333\n",
       "4            401    0.953333\n",
       "5            501    0.953333\n",
       "6            601    0.953333\n",
       "7            701    0.953333\n",
       "8            801    0.953333\n",
       "9            901    0.953333\n",
       "10          1001    0.953333"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = list(range(1, 1002, 100))\n",
    "\n",
    "n_estimators_out = []\n",
    "mean_scores  = []\n",
    "\n",
    "for n in estimators:\n",
    "    clf    = AdaBoostClassifier(n_estimators=n, algorithm=\"SAMME\")\n",
    "    scores = cross_val_score(estimator=clf, X=X, y=y, cv=5)\n",
    "    n_estimators_out.append(n)\n",
    "    mean_scores.append(scores.mean())\n",
    "\n",
    "pd.DataFrame(data=list(zip(n_estimators_out, mean_scores)), columns=['n_estimators', 'mean_score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
